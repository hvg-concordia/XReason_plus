{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ut6CFUAfLtsl"
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zd95ooRCMTnd"
   },
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJwCatpMHnca",
    "outputId": "14b9f415-8a37-47e7-a409-42a39b905022"
   },
   "outputs": [],
   "source": [
    "trained_model_lgbm = joblib.load('/content/drive/MyDrive/CIC/trained_model_lgbm.joblib')\n",
    "\n",
    "train_df = pd.read_csv('/content/drive/MyDrive/CIC/train_df.csv')\n",
    "X_train = train_df.drop('Labelb', axis=1)\n",
    "y_train = train_df['Labelb']\n",
    "\n",
    "test_df = pd.read_csv('/content/drive/MyDrive/CIC/test_df.csv')\n",
    "X_test = test_df.drop('Labelb', axis=1)\n",
    "y_test = test_df['Labelb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F44eX0sSB-N_",
    "outputId": "edc03fe2-796e-4bb7-8fe9-82b39ec89b5d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.array(X_train.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "shAECA2V4sfP",
    "outputId": "7ac45c26-900a-4358-b3b2-015a7288d73c"
   },
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEK41vy0T122"
   },
   "source": [
    "##Formal Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8MzNY_sWBxd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def DetermineFalsePredictions(df, all_features_list ,Targets, tolerance=1e-100):\n",
    "    csv_file = path_dataset\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df[df.columns[-1]] = Targets\n",
    "    df = df.iloc[:]\n",
    "    df_target = list(set(df[df.columns[-1]]))\n",
    "    print(df_target)\n",
    "    Verif_list = []\n",
    "    falsy_prediction = 0\n",
    "\n",
    "    # Conditions to match\n",
    "    for target, values in all_features_list.items():\n",
    "        for val in values:\n",
    "            conditions_to_check = val[1]\n",
    "            initial_class = df_target[target]\n",
    "\n",
    "            # Check if the target class matches\n",
    "            if np.isclose(df[df.columns[-1]][val[0]], initial_class, atol=tolerance):\n",
    "\n",
    "                # Apply condition filters with tolerance for floats\n",
    "                condition_filters = [(np.isclose(df[column], value, atol=tolerance) if isinstance(value, float)\n",
    "                                      else df[column] == value)\n",
    "                                     for column, value in conditions_to_check.items()]\n",
    "\n",
    "                # Combine the conditions\n",
    "                all_conditions = condition_filters[0]\n",
    "                for condition in condition_filters[1:]:\n",
    "                    all_conditions &= condition\n",
    "\n",
    "                # Filter the dataframe based on the combined conditions\n",
    "                filtered_df = df[all_conditions & (df[df.columns[-1]] != initial_class)]\n",
    "\n",
    "                if not filtered_df.empty:\n",
    "                    Verif_list.append([{initial_class: conditions_to_check}, filtered_df.iloc[0], filtered_df.index[0]])\n",
    "                    falsy_prediction += 1\n",
    "                else:\n",
    "                    Verif_list.append(\"correct\")\n",
    "            else:\n",
    "                    #  print(\"wrong prediction\")\n",
    "                     continue\n",
    "\n",
    "    print(\"There are\", falsy_prediction, \"false explanations over\", len(Verif_list))\n",
    "    return Verif_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TcLJ9jMrToDN",
    "outputId": "d8ab65b1-0593-4f4d-922a-f206f0e8c518"
   },
   "outputs": [],
   "source": [
    "path=\"explanations_cic_test.txt\"\n",
    "result,Targets,explanation_lengths=Determine_nb_features(path)\n",
    "print(\"Number of features in each instance for LGBM case:\", explanation_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y80NwGaWWQl2",
    "outputId": "e2eead3b-56bc-4ad1-acd6-c19e19c3a08c"
   },
   "outputs": [],
   "source": [
    "path_dataset=\"/content/drive/MyDrive/CIC/test_df.csv\"\n",
    "Verif_list_lgbm=DetermineFalsePredictions(path_dataset,result,Targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1Pa8XCpU_EL",
    "outputId": "72cd2b80-d46c-490f-c068-7c3ac8d15281"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Mean number of features:\", np.mean(explanation_lengths))\n",
    "print(\"Median number of features:\", np.median(explanation_lengths))\n",
    "print(\"Standard deviation of number of features:\", np.std(explanation_lengths))\n",
    "print(\"Minimum number of features:\", np.min(explanation_lengths))\n",
    "print(\"Maximum number of features:\", np.max(explanation_lengths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NhY4MdCKQdCi"
   },
   "outputs": [],
   "source": [
    "\n",
    "joblib.dump(all_features_indices, 'all_features_indices.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhurxAKoMYF1"
   },
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LWf6uTMnkou8",
    "outputId": "b649003f-dc60-489a-c019-a5c0252abed3"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "# Load the trained model\n",
    "trained_model_lgbm = joblib.load('/content/drive/MyDrive/CIC/trained_model_lgbm.joblib')\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('/content/drive/MyDrive/CIC/train_df.csv')\n",
    "X_train = train_df.drop('Labelb', axis=1)\n",
    "y_train = train_df['Labelb']\n",
    "\n",
    "# Make predictions on the training data\n",
    "y_pred = trained_model_lgbm.predict(X_train)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "print(\"Accuracy on train data:\", accuracy)\n",
    "\n",
    "test_df = pd.read_csv('/content/drive/MyDrive/CIC/test_df.csv')\n",
    "X_test = test_df.drop('Labelb', axis=1)\n",
    "y_test = test_df['Labelb']\n",
    "\n",
    "# Make predictions on the training data\n",
    "y_pred = trained_model_lgbm.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on test data:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_hGchAPT33uA",
    "outputId": "3fa6eb5b-7586-42d1-8610-d49db5326b74"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')  # Choose appropriate average method\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"AUC:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uIa5SPGEmV4n",
    "outputId": "bbfdf35c-b254-4d31-e6d9-cf8fba17b970"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# trained_model_lgbm = joblib.load('/content/drive/MyDrive/CIC/trained_model_lgbm.joblib')\n",
    "\n",
    "# test_df = pd.read_csv('/content/drive/MyDrive/CIC/test_df.csv')\n",
    "# X_test = test_df.drop('Labelb', axis=1)\n",
    "# y_test = test_df['Labelb']\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = trained_model_lgbm.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on test data:\", accuracy)\n",
    "\n",
    "# Generate a classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Generate a confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u6hrO2i7yWoZ",
    "outputId": "234452f1-c9c6-4199-d836-092d45060673"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "trained_model_xgb = joblib.load('/content/drive/MyDrive/CIC/cic_train_nbestim_10_maxdepth_3_testsplit_0.mod.pkl')\n",
    "\n",
    "train_df = pd.read_csv('/content/drive/MyDrive/CIC/train_df.csv')\n",
    "X_train = train_df.drop('Labelb', axis=1)\n",
    "y_train = train_df['Labelb']\n",
    "\n",
    "test_df = pd.read_csv('/content/drive/MyDrive/CIC/test_df.csv')\n",
    "X_test = test_df.drop('Labelb', axis=1)\n",
    "y_test = test_df['Labelb']\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on the training data\n",
    "y_pred_train = trained_model_xgb.predict(X_train)\n",
    "\n",
    "# Calculate the accuracy on the training data\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_test = trained_model_xgb.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy on the test data\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Accuracy on train data:\", accuracy_train)\n",
    "print(\"Accuracy on test data:\", accuracy_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxrlcXIdMBl-"
   },
   "source": [
    "## LIME explanation and Robustness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import shap\n",
    "from heuristic import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_lgbm = joblib.load('CICIDS_results/trained_model_lgbm.joblib')\n",
    "\n",
    "train_df = pd.read_csv('CICIDS_dataset/train_df.csv')\n",
    "X_train = train_df.drop('Labelb', axis=1)\n",
    "y_train = train_df['Labelb']\n",
    "\n",
    "test_df = pd.read_csv('CICIDS_dataset/test_df.csv')\n",
    "X_test = test_df.drop('Labelb', axis=1)\n",
    "y_test = test_df['Labelb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ytcik6C0hLZY"
   },
   "outputs": [],
   "source": [
    "# Create a LimeTabularExplainer\n",
    "explainer1 = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=np.array(X_train),\n",
    "    feature_names=X_train.columns,\n",
    "    class_names=[str(i) for i in np.unique(y_train)],\n",
    "    mode=\"classification\",\n",
    "    discretize_continuous=False\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "R-Zuw6H4RbMS"
   },
   "outputs": [],
   "source": [
    "# Create another LimeTabularExplainer\n",
    "\n",
    "explainer2 = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=np.array(X_train),\n",
    "    feature_names=X_train.columns,\n",
    "    class_names=[str(i) for i in np.unique(y_train)],\n",
    "    mode=\"classification\",\n",
    "    discretize_continuous=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MdU4Y58tRfKF",
    "outputId": "af99c0de-591d-47b9-be30-2a48729a5937"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The explanations generated by LIME are not the same for a given instance\n"
     ]
    }
   ],
   "source": [
    "if explainer1==explainer2:\n",
    "    print(\"The explanations generated by LIME are consistent for a given instance.\")\n",
    "else:\n",
    "    print(\"The explanations generated by LIME are not the same for a given instance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVxFDebtRnlG"
   },
   "outputs": [],
   "source": [
    "all_ranked_features1=[]\n",
    "all_ranked_features2=[]\n",
    "nb_similar_explanations=0\n",
    "# Generate explanations for all instances\n",
    "for i in range(len(X_test)):\n",
    "    all_ranked_features2_instance=[]\n",
    "    all_ranked_features1_instance=[]\n",
    "    exp1 = explainer1.explain_instance(X_test.iloc[i], trained_model_lgbm.predict_proba, num_features=len(X_test.columns))\n",
    "    exp2 = explainer2.explain_instance(X_test.iloc[i], trained_model_lgbm.predict_proba, num_features=len(X_test.columns))\n",
    "    # Get feature importance scores\n",
    "    feature_scores1 = exp1.as_map()[exp1.available_labels()[0]]\n",
    "    feature_scores2 = exp2.as_map()[exp2.available_labels()[0]]\n",
    "    if (feature_scores1==feature_scores2):\n",
    "      nb_similar_explanations+=1\n",
    "    # Rank features based on absolute scores\n",
    "    ranked_features1 = sorted(feature_scores1, key=lambda x: abs(x[1]), reverse=True)\n",
    "    ranked_features2 = sorted(feature_scores2, key=lambda x: abs(x[1]), reverse=True)\n",
    "    # print(f\"\\nExplanation for instance {i + 1}:\")\n",
    "    for feature, score in ranked_features1:\n",
    "        # print(f\"Feature: {X_test.columns[feature]}, Score: {score}\")\n",
    "        all_ranked_features1_instance.extend([feature])\n",
    "    for feature, score in ranked_features2:\n",
    "        # print(f\"Feature: {X_test.columns[feature]}, Score: {score}\")\n",
    "        all_ranked_features2_instance.extend([feature])\n",
    "    all_ranked_features1.append(all_ranked_features1_instance)\n",
    "    all_ranked_features2.append(all_ranked_features2_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hChzS0VkUJad",
    "outputId": "05cd29fb-e107-4495-afa8-79b887a8ee43"
   },
   "outputs": [],
   "source": [
    "# Save the all_ranked_features list to a file\n",
    "joblib.dump(all_ranked_features, 'all_ranked_features_LIME.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XUP6wZFEeDkV"
   },
   "outputs": [],
   "source": [
    "all_ranked_features = joblib.load('CICIDS_results/all_ranked_features_LIME.joblib')\n",
    "all_features_indices = joblib.load('CICIDS_results/all_features_indices.joblib')\n",
    "all_ranked_features_SHAP = joblib.load('CICIDS_results/all_ranked_features_SHAP.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7213z3iHLJyf"
   },
   "source": [
    "## Correctness of LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAdeiPgoVsmE"
   },
   "source": [
    "Note: we should add 1 to all ranks of all_ranked_features of LIME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Hkjd89efV2l7"
   },
   "outputs": [],
   "source": [
    "tool2_ranks_plus_one = [[element + 1 for element in sublist] for sublist in all_ranked_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "V4zpPT74Vcdq"
   },
   "outputs": [],
   "source": [
    "#calculate_spearman_correlation\n",
    "\n",
    "total_features = len(X_test.columns)\n",
    "spearman_values,spearman_p_values=[],[]\n",
    "# Tool 1: Selects 5 most important features (considered equally important)\n",
    "tool1_selection = all_features_indices # Indices of selected features\n",
    "# Tool 2: Assigns ranks to all features \n",
    "tool2_ranks = tool2_ranks_plus_one\n",
    "for i in range(len(tool1_selection)):\n",
    "  correlation,p_value=calculate_spearman_correlation(tool1_selection[i], tool2_ranks[i],total_features)\n",
    "  spearman_values.append(correlation)\n",
    "  spearman_p_values.append(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "u6K3sPBBbWST"
   },
   "outputs": [],
   "source": [
    "#calculate_kendall_tau_correlation\n",
    "\n",
    "total_features = len(X_test.columns)\n",
    "# Tool 1: Selects 5 most important features (considered equally important)\n",
    "tool1_selection = all_features_indices # Indices of selected features\n",
    "# Tool 2: Assigns ranks to all features (1 is most important, 10 is least important)\n",
    "tool2_ranks = tool2_ranks_plus_one\n",
    "tau_values,tau_p_values=[],[]\n",
    "for i in range(len(tool1_selection)):\n",
    "  tau,p_value=calculate_kendall_tau(tool1_selection[i], tool2_ranks[i],total_features)\n",
    "  tau_values.append(tau)\n",
    "  tau_p_values.append(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aQ5iRKFyJNCR",
    "outputId": "c3fe0127-715d-4603-8c23-133f3d370913"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum spearman value: 0.7201190377787747\n",
      "Minimum spearman value: -0.6982972487551756\n",
      "Average spearman value: 0.1810978595313205\n",
      "--------------------------------------------------\n",
      "Maximum spearman p_value: 1.0\n",
      "Minimum spearman p_value: 0.0005071855544230693\n",
      "Average spearman p_value: 0.39927230358734356\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Maximum spearman value:\", max(spearman_values))\n",
    "print(\"Minimum spearman value:\", min(spearman_values))\n",
    "print(\"Average spearman value:\", sum(spearman_values) / len(spearman_values))\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"Maximum spearman p_value:\", max(spearman_p_values))\n",
    "print(\"Minimum spearman p_value:\", min(spearman_p_values))\n",
    "print(\"Average spearman p_value:\", sum(spearman_p_values) / len(spearman_p_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "htMPRg1YbqsD",
    "outputId": "ebbce774-6cde-4a69-dc11-55ff63ca2e5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum tau value: 0.60324934681779\n",
      "Minimum tau value: -0.5849690635808872\n",
      "Average tau value: 0.15170709249591216\n",
      "--------------------------------------------------\n",
      "Maximum kendall p_value: 1.0\n",
      "Minimum kendall p_value: 0.0022490573909353163\n",
      "Average kendall p_value: 0.39309846976031715\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Maximum tau value:\", max(tau_values))\n",
    "print(\"Minimum tau value:\", min(tau_values))\n",
    "print(\"Average tau value:\", sum(tau_values) / len(tau_values))\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"Maximum kendall p_value:\", max(tau_p_values))\n",
    "print(\"Minimum kendall p_value:\", min(tau_p_values))\n",
    "print(\"Average kendall p_value:\", sum(tau_p_values) / len(tau_p_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfB_jItaLMzE"
   },
   "source": [
    "## SHAP explanation and Robustness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3lYDm21HQs3L"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create a SHAP explainer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m explainer1 \u001b[38;5;241m=\u001b[39m \u001b[43mshap\u001b[49m\u001b[38;5;241m.\u001b[39mExplainer(model)\n\u001b[1;32m      3\u001b[0m explainer2 \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mExplainer(model)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Calculate SHAP values for all test samples\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'shap' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a SHAP explainer\n",
    "explainer1 = shap.Explainer(model)\n",
    "explainer2 = shap.Explainer(model)\n",
    "\n",
    "# Calculate SHAP values for all test samples\n",
    "shap_values1 = explainer1(X_test)\n",
    "shap_values2 = explainer2(X_test)\n",
    "\n",
    "# Generate rankings for all samples\n",
    "all_rankings = []\n",
    "for i in range(len(X_test)):\n",
    "    sample_rankings = shap_to_rankings(shap_values[i].values)\n",
    "    all_rankings.append(sample_rankings)\n",
    "# Save the all_rankings list to a file\n",
    "joblib.dump(all_rankings, 'all_ranked_features_SHAP.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5G-L5fi-URp5"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Create a SHAP explainer\n",
    "explainer1 = shap.Explainer(trained_model_lgbm)\n",
    "explainer2 = shap.Explainer(trained_model_lgbm)\n",
    "\n",
    "# Calculate SHAP values for all test samples\n",
    "shap_values1 = explainer1(X_test)\n",
    "shap_values2 = explainer2(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6BnnUxEiXGS5",
    "outputId": "af495a37-7004-42ca-fc35-28fcbe074ae0"
   },
   "outputs": [],
   "source": [
    "\n",
    "nb_similar_explanations = 0\n",
    "for i in range(len(shap_values1)):\n",
    "  if (shap_values1[i].values == shap_values2[i].values).all():\n",
    "    nb_similar_explanations += 1\n",
    "\n",
    "percentage_similar = (nb_similar_explanations / len(shap_values1)) * 100\n",
    "print(f\"Percentage of similar explanations: {percentage_similar:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCXtiFo4Ghpn"
   },
   "source": [
    "## Correctness of SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dudyd3_oHRU2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "total_features = len(X_test.columns)\n",
    "spearman_values,spearman_p_values=[],[]\n",
    "\n",
    "# Tool 1: Selects 5 most important features (considered equally important)\n",
    "tool1_selection = all_features_indices # Indices of selected features\n",
    "\n",
    "tool2_ranks = all_ranked_features_SHAP\n",
    "for i in range(len(tool1_selection)):\n",
    "  correlation,p_value=calculate_spearman_correlation(tool1_selection[i], tool2_ranks[i])\n",
    "  spearman_values.append(correlation)\n",
    "  spearman_p_values.append(p_value)\n",
    "tau_values,tau_p_values=[],[]\n",
    "for i in range(len(tool1_selection)):\n",
    "  tau,p_value=calculate_kendall_tau(tool1_selection[i], tool2_ranks[i])\n",
    "  tau_values.append(tau)\n",
    "  tau_p_values.append(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pX5j2Hv_IOKA",
    "outputId": "a21640b1-927b-4bdb-94b9-a72c965d347f"
   },
   "outputs": [],
   "source": [
    "print(\"Maximum spearman value:\", max(spearman_values))\n",
    "print(\"Minimum spearman value:\", min(spearman_values))\n",
    "print(\"Average spearman value:\", sum(spearman_values) / len(spearman_values))\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"Maximum spearman p_value:\", max(spearman_p_values))\n",
    "print(\"Minimum spearman p_value:\", min(spearman_p_values))\n",
    "print(\"Average spearman p_value:\", sum(spearman_p_values) / len(spearman_p_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lDE2JXcEIOKB",
    "outputId": "a6196a44-7778-467e-ad68-cabbc7fc4f76"
   },
   "outputs": [],
   "source": [
    "print(\"Maximum tau value:\", max(tau_values))\n",
    "print(\"Minimum tau value:\", min(tau_values))\n",
    "print(\"Average tau value:\", sum(tau_values) / len(tau_values))\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"Maximum kendall p_value:\", max(tau_p_values))\n",
    "print(\"Minimum kendall p_value:\", min(tau_p_values))\n",
    "print(\"Average kendall p_value:\", sum(tau_p_values) / len(tau_p_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKv9Xq7Xa_zn"
   },
   "source": [
    "## RBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGzgBbnWh2NZ"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the all_ranked_features list from the file\n",
    "all_ranked_features_LIME = joblib.load('all_ranked_features_LIME.joblib')\n",
    "all_ranked_features_SHAP = joblib.load('all_ranked_features_SHAP.joblib')\n",
    "all_features_indices= joblib.load('all_features_indices.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5DDDBAGbEEt"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def rbo(list1, list2, p=0.9):\n",
    "    \"\"\"\n",
    "    Calculates Rank-Biased Overlap (RBO) between two lists.\n",
    "\n",
    "    Args:\n",
    "    - list1, list2: lists to be compared\n",
    "    - p: persistence parameter (default 0.9)\n",
    "\n",
    "    Returns:\n",
    "    - RBO score\n",
    "    \"\"\"\n",
    "    if not 0 <= p <= 1:\n",
    "        raise ValueError(\"p must be between 0 and 1.\")\n",
    "\n",
    "    # Convert lists to sets for easier intersection\n",
    "    set1, set2 = set(list1), set(list2)\n",
    "\n",
    "    # Get the longer list\n",
    "    max_length = max(len(list1), len(list2))\n",
    "\n",
    "    # Calculate the agreement at each depth\n",
    "    score = 0.0\n",
    "    intersect_size = 0\n",
    "\n",
    "    for d in range(1, max_length + 1):\n",
    "        intersect_size += len(set(list1[:d]) & set(list2[:d])) - len(set(list1[:d-1]) & set(list2[:d-1]))\n",
    "        agreement = intersect_size / d\n",
    "        score += agreement * math.pow(p, d - 1)\n",
    "\n",
    "    # Normalize the score\n",
    "    norm = (1 - p) / p\n",
    "    return score * norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qNo8Y0J6iHnn",
    "outputId": "1e91c24e-0f60-4cbe-caa7-d8c751aa5ab3"
   },
   "outputs": [],
   "source": [
    "\n",
    "rbo_scores = []\n",
    "for i in range(len(all_ranked_features_LIME)):\n",
    "  rbo_score = rbo(all_ranked_features_LIME[i], all_features_indices[i])\n",
    "  rbo_scores.append(rbo_score)\n",
    "\n",
    "print(\"Maximum RBO score:\", max(rbo_scores))\n",
    "print(\"Minimum RBO score:\", min(rbo_scores))\n",
    "print(\"Average RBO score:\", sum(rbo_scores) / len(rbo_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4L6V37zvhzmC",
    "outputId": "0515b35c-a5db-4fc9-e509-f7ff75bec760"
   },
   "outputs": [],
   "source": [
    "\n",
    "rbo_scores = []\n",
    "for i in range(len(all_ranked_features_SHAP)):\n",
    "  rbo_score = rbo(all_ranked_features_SHAP[i], all_features_indices[i])\n",
    "  rbo_scores.append(rbo_score)\n",
    "\n",
    "print(\"Maximum RBO score:\", max(rbo_scores))\n",
    "print(\"Minimum RBO score:\", min(rbo_scores))\n",
    "print(\"Average RBO score:\", sum(rbo_scores) / len(rbo_scores))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
